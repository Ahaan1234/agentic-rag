{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "377e468e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in ./.venv/lib/python3.13/site-packages (0.6.1)\n",
      "Requirement already satisfied: langchain_openai in ./.venv/lib/python3.13/site-packages (0.3.28)\n",
      "Collecting langfuse\n",
      "  Downloading langfuse-3.2.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: langchain-core>=0.1 in ./.venv/lib/python3.13/site-packages (from langgraph) (0.3.70)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in ./.venv/lib/python3.13/site-packages (from langgraph) (2.1.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in ./.venv/lib/python3.13/site-packages (from langgraph) (0.6.1)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.0 in ./.venv/lib/python3.13/site-packages (from langgraph) (0.2.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in ./.venv/lib/python3.13/site-packages (from langgraph) (2.11.7)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in ./.venv/lib/python3.13/site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in ./.venv/lib/python3.13/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in ./.venv/lib/python3.13/site-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in ./.venv/lib/python3.13/site-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (3.11.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in ./.venv/lib/python3.13/site-packages (from langchain_openai) (1.97.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./.venv/lib/python3.13/site-packages (from langchain_openai) (0.9.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in ./.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (0.4.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in ./.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.13/site-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain_openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.13/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.venv/lib/python3.13/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.4)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./.venv/lib/python3.13/site-packages (from langfuse) (2.2.1)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from langfuse)\n",
      "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp<2.0.0,>=1.33.1 (from langfuse)\n",
      "  Downloading opentelemetry_exporter_otlp-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from langfuse)\n",
      "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting packaging>=23.2 (from langchain-core>=0.1->langgraph)\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.14 in ./.venv/lib/python3.13/site-packages (from langfuse) (1.17.2)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in ./.venv/lib/python3.13/site-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.13/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse) (3.23.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.36.0 (from opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.36.0 (from opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in ./.venv/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.66.2 in ./.venv/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse) (1.73.1)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse)\n",
      "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in ./.venv/lib/python3.13/site-packages (from opentelemetry-proto==1.36.0->opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse) (5.29.5)\n",
      "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse)\n",
      "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.5.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.23.0)\n",
      "Downloading langfuse-3.2.1-py3-none-any.whl (299 kB)\n",
      "Downloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_exporter_otlp-1.36.0-py3-none-any.whl (7.0 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
      "Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Installing collected packages: packaging, opentelemetry-proto, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, opentelemetry-semantic-conventions, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-exporter-otlp, langfuse\n",
      "\u001b[2K  Attempting uninstall: packaging\n",
      "\u001b[2K    Found existing installation: packaging 25.0\n",
      "\u001b[2K    Uninstalling packaging-25.0:\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [langfuse]/10\u001b[0m [langfuse]ter-otlp-proto-grpc]\n",
      "\u001b[1A\u001b[2KSuccessfully installed langfuse-3.2.1 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-exporter-otlp-proto-http-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 packaging-24.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langgraph langchain_openai langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be38331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from typing import TypedDict, List, Dict, Any, Optional\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from smolagents import LiteLLMModel\n",
    "from langchain.schema import BaseMessage, ChatMessage\n",
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d9ff92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Authentication error: Langfuse client initialized without public_key. Client will be disabled. Provide a public_key parameter or set LANGFUSE_PUBLIC_KEY environment variable. \n"
     ]
    }
   ],
   "source": [
    "class EmailState(TypedDict):\n",
    "    email: Dict[str, Any]\n",
    "    is_spam: Optional[bool]\n",
    "    spam_reason: Optional[str]\n",
    "    email_category: Optional[str]\n",
    "    email_draft: Optional[str]\n",
    "    messages: List[Dict[str, Any]]\n",
    "\n",
    "from langfuse.langchain import CallbackHandler\n",
    "lf_handler = CallbackHandler()\n",
    "\n",
    "model = LiteLLMModel(\n",
    "    model_id=\"gemini/gemini-2.0-flash-lite\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=1024,\n",
    "    callbacks=[lf_handler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b809f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_email(state: EmailState):\n",
    "    \"\"\"Agent reads and logs the incoming email\"\"\"\n",
    "    email = state['email']\n",
    "    # Here we might do some initial preprocessing\n",
    "    print(f\"Alfred is processing an email from {email['sender']} with subject: {email['subject']}\")\n",
    "\n",
    "    return{}\n",
    "\n",
    "def classify_email(state: EmailState):\n",
    "    \"\"\"Agent uses an LLM to determine whether an email is spam or legitamate\"\"\"\n",
    "    email = state['email']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    As my agent, analyze this email and determine if it is spam or legitimate.\n",
    "\n",
    "    Analyse the e-mail below.\n",
    "    1. Say whether it is spam.\n",
    "    2. If spam, explain why. \n",
    "    3. If not spam, label it with a broad category (inquiry, complaint, thank-you, request, info).\n",
    "    \n",
    "    Email:\n",
    "    From: {email['sender']}\n",
    "    Subject: {email['subject']}\n",
    "    Body: {email['body']}\n",
    "    \"\"\"\n",
    "\n",
    "    messages=[HumanMessage(content=prompt)]\n",
    "    response = model.generate([messages])\n",
    "    response_text = response.content.lower()\n",
    "    \n",
    "    is_spam=\"spam\" in response_text and \"not spam\" not in resresponse_textponsetext\n",
    "\n",
    "    spam_reason = None\n",
    "    if is_spam and \"reason: \" in response_text:\n",
    "        spam_reason = response_text.split(\"reason:\")[1].strip()\n",
    "\n",
    "    email_cat = None\n",
    "    if not is_spam:\n",
    "        categories = [\"inquiry\", \"complaint\", \"thank you\", \"request\", \"information\"]\n",
    "        for category in categories:\n",
    "            if category in response_text:\n",
    "                email_category = category\n",
    "                break\n",
    "    \n",
    "    new_message = state.get(\"messages\", []) + [\n",
    "        {\"role\":\"user\", \"content\":prompt},\n",
    "        {\"role\":\"assisstant\", \"content\":response.content}\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"is_spam\": is_spam,\n",
    "        \"spam_reason\": spam_reason,\n",
    "        \"email_category\": email_category,\n",
    "        \"messages\": new_message,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a2c9ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_spam(state: EmailState):\n",
    "    \"\"\"Agent discards spam email with a note\"\"\"\n",
    "    print(f\"Agent has marked the email as spam. Reason: {state['spam_reason']}\")\n",
    "    print(\"The email has been moved to the spam folder.\")\n",
    "    \n",
    "    return {}\n",
    "\n",
    "def draft_response(state: EmailState):\n",
    "    \"\"\"Agent drafts a preliminary response for legitimate emails\"\"\"\n",
    "    email = state[\"email\"]\n",
    "    category = state[\"email_category\"] or \"general\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    As my Agent, draft a polite preliminary response to this email.\n",
    "    \n",
    "    Email:\n",
    "    From: {email['sender']}\n",
    "    Subject: {email['subject']}\n",
    "    Body: {email['body']}\n",
    "    \n",
    "    This email has been categorized as: {category}\n",
    "    \n",
    "    Draft a brief, professional response that I can review and personalize before sending.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [HumanMessage(content = prompt)]\n",
    "    response = model.generate(messages)\n",
    "\n",
    "    new_msgs = state[\"messages\"] + [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response.content},\n",
    "    ]\n",
    "    return {\"email_draft\": response.content, \"messages\": new_msgs}\n",
    "\n",
    "def notify_me(state: EmailState):\n",
    "    \"\"\"\n",
    "    Notify me when an email comes in along with its classification\n",
    "    \"\"\"\n",
    "    e = state[\"email\"]\n",
    "    print(f\"\\nSir, new mail from {e['sender']} ({state['email_category']})\")\n",
    "    print(state[\"email_draft\"])\n",
    "    return {}\n",
    "\n",
    "def route_email(state: EmailState) -> str:\n",
    "    return \"spam\" if state[\"is_spam\"] else \"legitimate\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = StateGraph(EmailState)\n",
    "\n",
    "g.add_node(\"read_email\", read_email)\n",
    "g.add_node(\"classify_email\", classify_email)\n",
    "g.add_node(\"handle_spam\", handle_spam)\n",
    "g.add_node(\"draft_response\", draft_response)\n",
    "g.add_node(\"notify_mr_wayne\", notify_me)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
